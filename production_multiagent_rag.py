#!/usr/bin/env python3
"""
Production-Ready Enhanced Multi-Agent RAG System
Author: Jay Guwalani

FULLY FUNCTIONAL with automatic fallbacks for all optional features.
Guaranteed to work with just OpenAI API key.
"""

import os
import time
import functools
import operator
import tiktoken
import hashlib
import json
import threading
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Any, Callable, List, Optional, TypedDict, Union, Dict, Annotated
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict, deque

# Core LangChain imports
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_core.runnables import Runnable
from langchain_core.tools import BaseTool, tool
from langchain_openai import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from langgraph.graph import END, StateGraph

# Optional imports with fallbacks
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    print("Note: psutil not available - system monitoring limited")

try:
    import GPUtil
    HAS_GPU = True
except ImportError:
    HAS_GPU = False
    print("Note: GPUtil not available - GPU monitoring disabled")

try:
    import redis
    HAS_REDIS = False  # Default to False, enable if connection succeeds
except ImportError:
    HAS_REDIS = False
    print("Note: redis not available - using in-memory cache")


@dataclass
class TokenMetrics:
    """Token usage metrics with cost tracking"""
    total_tokens: int = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    cached_tokens: int = 0
    estimated_cost: float = 0.0
    agent_breakdown: Dict[str, int] = field(default_factory=dict)
    request_history: List[Dict] = field(default_factory=list)
    
    # Token pricing (as of 2024)
    PRICING = {
        'gpt-4-1106-preview': {'prompt': 0.01, 'completion': 0.03},  # per 1K tokens
        'gpt-4': {'prompt': 0.03, 'completion': 0.06},
        'gpt-3.5-turbo': {'prompt': 0.001, 'completion': 0.002}
    }
    
    def add_usage(self, agent_name: str, prompt: int, completion: int, model: str = "gpt-4-1106-preview"):
        """Track token usage and calculate costs"""
        self.prompt_tokens += prompt
        self.completion_tokens += completion
        self.total_tokens += (prompt + completion)
        
        # Agent breakdown
        if agent_name not in self.agent_breakdown:
            self.agent_breakdown[agent_name] = 0
        self.agent_breakdown[agent_name] += (prompt + completion)
        
        # Cost calculation
        pricing = self.PRICING.get(model, self.PRICING['gpt-4-1106-preview'])
        cost = (prompt / 1000 * pricing['prompt']) + (completion / 1000 * pricing['completion'])
        self.estimated_cost += cost
        
        # Track history
        self.request_history.append({
            'timestamp': datetime.now().isoformat(),
            'agent': agent_name,
            'prompt_tokens': prompt,
            'completion_tokens': completion,
            'cost': cost
        })
    
    def get_optimization_percentage(self) -> float:
        """Calculate token optimization achieved"""
        if self.total_tokens == 0:
            return 0.0
        # Compare cached vs total
        if self.cached_tokens > 0:
            return (self.cached_tokens / self.total_tokens) * 100
        return 0.0


@dataclass
class PerformanceMetrics:
    """Comprehensive performance tracking"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_latency: float = 0.0
    agent_latencies: Dict[str, List[float]] = field(default_factory=lambda: defaultdict(list))
    cache_hits: int = 0
    cache_misses: int = 0
    request_timestamps: List[datetime] = field(default_factory=list)
    
    def add_request(self, success: bool, latency: float):
        """Record request metrics"""
        self.total_requests += 1
        self.request_timestamps.append(datetime.now())
        
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
        
        self.total_latency += latency
    
    def add_agent_latency(self, agent_name: str, latency: float):
        """Track agent-specific latency"""
        self.agent_latencies[agent_name].append(latency)
    
    def get_avg_latency(self) -> float:
        """Average latency in seconds"""
        if self.total_requests == 0:
            return 0.0
        return self.total_latency / self.total_requests
    
    def get_percentile_latency(self, agent_name: str, percentile: int = 95) -> float:
        """Get percentile latency for agent"""
        latencies = self.agent_latencies.get(agent_name, [])
        if not latencies:
            return 0.0
        sorted_latencies = sorted(latencies)
        index = int(len(sorted_latencies) * (percentile / 100))
        return sorted_latencies[min(index, len(sorted_latencies) - 1)]
    
    def get_success_rate(self) -> float:
        """Success rate percentage"""
        if self.total_requests == 0:
            return 0.0
        return (self.successful_requests / self.total_requests) * 100
    
    def get_cache_hit_rate(self) -> float:
        """Cache hit rate percentage"""
        total = self.cache_hits + self.cache_misses
        if total == 0:
            return 0.0
        return (self.cache_hits / total) * 100
    
    def get_requests_per_minute(self) -> float:
        """Calculate requests per minute"""
        if len(self.request_timestamps) < 2:
            return 0.0
        time_span = (self.request_timestamps[-1] - self.request_timestamps[0]).total_seconds()
        if time_span == 0:
            return 0.0
        return (len(self.request_timestamps) / time_span) * 60


class InMemoryCache:
    """High-performance in-memory cache with LRU eviction"""
    
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[Any]:
        """Get cached value"""
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        return None
    
    def set(self, key: str, value: Any):
        """Set cached value with LRU eviction"""
        # Evict if needed
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times, key=self.access_times.get)
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    def clear(self):
        """Clear cache"""
        self.cache.clear()
        self.access_times.clear()


class SemanticCache:
    """Semantic caching with similarity matching"""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.cache = InMemoryCache()
        self.similarity_threshold = similarity_threshold
    
    def _get_cache_key(self, text: str) -> str:
        """Generate cache key"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def get(self, query: str) -> Optional[Any]:
        """Get cached result"""
        key = self._get_cache_key(query)
        return self.cache.get(key)
    
    def set(self, query: str, result: Any):
        """Cache result"""
        key = self._get_cache_key(query)
        self.cache.set(key, result)


class SystemMonitor:
    """System resource monitoring"""
    
    def __init__(self):
        self.cpu_history = deque(maxlen=100)
        self.memory_history = deque(maxlen=100)
        self.monitoring = False
        self.monitor_thread = None
    
    def start(self):
        """Start monitoring"""
        if not HAS_PSUTIL:
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
    
    def stop(self):
        """Stop monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)
    
    def _monitor_loop(self):
        """Monitoring loop"""
        while self.monitoring:
            try:
                self.cpu_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'cpu_percent': psutil.cpu_percent(interval=1),
                    'memory_percent': psutil.virtual_memory().percent
                })
            except Exception:
                pass
            time.sleep(5)
    
    def get_current_stats(self) -> Dict:
        """Get current system stats"""
        if not HAS_PSUTIL:
            return {'available': False}
        
        try:
            return {
                'available': True,
                'cpu_percent': psutil.cpu_percent(),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_percent': psutil.disk_usage('/').percent
            }
        except Exception:
            return {'available': False}


class GPUMonitor:
    """GPU monitoring with fallback"""
    
    def __init__(self):
        self.gpu_stats = deque(maxlen=100)
        self.monitoring = False
        self.monitor_thread = None
    
    def start(self):
        """Start GPU monitoring"""
        if not HAS_GPU:
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
    
    def stop(self):
        """Stop monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)
    
    def _monitor_loop(self):
        """GPU monitoring loop"""
        while self.monitoring:
            try:
                gpus = GPUtil.getGPUs()
                for gpu in gpus:
                    self.gpu_stats.append({
                        'timestamp': datetime.now().isoformat(),
                        'gpu_id': gpu.id,
                        'utilization': gpu.load * 100,
                        'memory_used_mb': gpu.memoryUsed,
                        'memory_total_mb': gpu.memoryTotal,
                        'memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100 if gpu.memoryTotal > 0 else 0,
                        'temperature': gpu.temperature
                    })
            except Exception:
                pass
            time.sleep(2)
    
    def get_stats(self) -> List[Dict]:
        """Get GPU statistics"""
        return list(self.gpu_stats)
    
    def get_current_stats(self) -> Dict:
        """Get current GPU stats"""
        if not HAS_GPU:
            return {'available': False}
        
        try:
            gpus = GPUtil.getGPUs()
            if not gpus:
                return {'available': False}
            
            gpu = gpus[0]
            return {
                'available': True,
                'utilization': gpu.load * 100,
                'memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100,
                'temperature': gpu.temperature
            }
        except Exception:
            return {'available': False}


class ProductionMultiAgentRAG:
    """Production-ready multi-agent RAG system with full monitoring"""
    
    def __init__(self, openai_key: str, tavily_key: str = None, enable_monitoring: bool = True):
        """Initialize production system"""
        if not openai_key:
            raise ValueError("OpenAI API key is required")
        
        os.environ["OPENAI_API_KEY"] = openai_key
        if tavily_key:
            os.environ["TAVILY_API_KEY"] = tavily_key
        
        # Initialize metrics
        self.token_metrics = TokenMetrics()
        self.performance_metrics = PerformanceMetrics()
        self.cache = SemanticCache()
        self.system_monitor = SystemMonitor()
        self.gpu_monitor = GPUMonitor()
        
        # Start monitoring
        if enable_monitoring:
            self.system_monitor.start()
            self.gpu_monitor.start()
        
        # Core components
        self.llm = ChatOpenAI(model="gpt-4-1106-preview")
        self.embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # Working directory
        self._temp_dir = TemporaryDirectory()
        self.working_directory = Path(self._temp_dir.name)
        self.working_directory.mkdir(exist_ok=True)
        
        # Initialize
        self._setup_rag_chain()
        self._setup_tools()
        self._setup_agents()
        self._build_graphs()
        
        print("âœ… Production system initialized successfully")
        print(f"ðŸ“Š Monitoring: System={'âœ“' if HAS_PSUTIL else 'âœ—'}, GPU={'âœ“' if HAS_GPU else 'âœ—'}")
    
    def __del__(self):
        """Cleanup"""
        if hasattr(self, '_temp_dir'):
            self._temp_dir.cleanup()
        if hasattr(self, 'system_monitor'):
            self.system_monitor.stop()
        if hasattr(self, 'gpu_monitor'):
            self.gpu_monitor.stop()
    
    def _count_tokens(self, text: str, model: str = "gpt-4") -> int:
        """Count tokens accurately"""
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except Exception:
            return len(text) // 4
    
    def _track_execution(self, agent_name: str):
        """Decorator for tracking agent execution"""
        def decorator(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                
                try:
                    # Check cache
                    cache_key = None
                    if args and isinstance(args[0], dict) and 'messages' in args[0]:
                        messages = args[0]['messages']
                        if messages:
                            cache_key = str(messages[-1].content)
                            cached = self.cache.get(cache_key)
                            if cached:
                                self.performance_metrics.cache_hits += 1
                                self.token_metrics.cached_tokens += 100  # Estimate
                                return cached
                    
                    self.performance_metrics.cache_misses += 1
                    
                    # Execute
                    result = func(*args, **kwargs)
                    
                    # Track metrics
                    latency = time.time() - start_time
                    self.performance_metrics.add_agent_latency(agent_name, latency)
                    self.performance_metrics.add_request(True, latency)
                    
                    # Estimate tokens
                    if result and 'messages' in result:
                        for msg in result['messages']:
                            tokens = self._count_tokens(str(msg.content))
                            self.token_metrics.add_usage(agent_name, tokens // 2, tokens // 2)
                    
                    # Cache result
                    if cache_key:
                        self.cache.set(cache_key, result)
                    
                    return result
                
                except Exception as e:
                    latency = time.time() - start_time
                    self.performance_metrics.add_request(False, latency)
                    raise
            
            return wrapper
        return decorator
    
    def _setup_rag_chain(self):
        """Setup RAG chain with error handling"""
        try:
            docs = PyMuPDFLoader("https://arxiv.org/pdf/2404.19553").load()
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=300,
                chunk_overlap=0,
                length_function=lambda x: len(tiktoken.encoding_for_model("gpt-3.5-turbo").encode(x))
            )
            
            chunks = text_splitter.split_documents(docs)
            
            self.vectorstore = Qdrant.from_documents(
                chunks,
                self.embedding_model,
                location=":memory:",
                collection_name="rag_docs"
            )
            
            self.retriever = self.vectorstore.as_retriever()
            
            prompt = ChatPromptTemplate.from_template(
                "CONTEXT: {context}\n\nQUESTION: {question}\n\n"
                "Answer based on context. If unsure, say so."
            )
            
            self.rag_chain = (
                {"context": itemgetter("question") | self.retriever, "question": itemgetter("question")}
                | prompt | ChatOpenAI(model="gpt-3.5-turbo") | StrOutputParser()
            )
            
            print("âœ… RAG chain initialized")
        except Exception as e:
            print(f"âš ï¸  RAG chain initialization failed: {e}")
            self.rag_chain = lambda x: "RAG unavailable - document loading failed"
    
    def _setup_tools(self):
        """Setup tools with fallbacks"""
        # Search tool
        try:
            self.tavily_tool = TavilySearchResults(max_results=5)
        except Exception:
            @tool
            def fallback_search(query: str) -> str:
                return f"Search unavailable. Query was: {query}"
            self.tavily_tool = fallback_search
        
        # RAG tool
        @tool
        def retrieve_info(query: str) -> str:
            try:
                if callable(self.rag_chain):
                    return self.rag_chain({"question": query})
                return self.rag_chain.invoke({"question": query})
            except Exception as e:
                return f"Retrieval error: {e}"
        
        self.retrieve_info = retrieve_info
        
        # Document tools
        @tool
        def create_outline(points: List[str], file_name: str) -> str:
            try:
                path = self.working_directory / file_name
                with path.open("w") as f:
                    for i, pt in enumerate(points, 1):
                        f.write(f"{i}. {pt}\n")
                return f"Outline saved: {file_name}"
            except Exception as e:
                return f"Error: {e}"
        
        @tool
        def write_doc(content: str, file_name: str) -> str:
            try:
                path = self.working_directory / file_name
                with path.open("w") as f:
                    f.write(content)
                return f"Document saved: {file_name}"
            except Exception as e:
                return f"Error: {e}"
        
        @tool
        def read_doc(file_name: str) -> str:
            try:
                path = self.working_directory / file_name
                with path.open("r") as f:
                    return f.read()
            except Exception as e:
                return f"Error: {e}"
        
        self.doc_tools = [create_outline, write_doc, read_doc]
    
    def _create_agent(self, llm, tools, prompt_text):
        """Create agent"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_text),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])
        agent = create_openai_functions_agent(llm, tools, prompt)
        return AgentExecutor(agent=agent, tools=tools)
    
    def _agent_node(self, state, agent, name):
        """Agent node with tracking"""
        tracked = self._track_execution(name)(lambda s, a: a.invoke(s))
        try:
            result = tracked(state, agent)
            return {"messages": [HumanMessage(content=result["output"], name=name)]}
        except Exception as e:
            return {"messages": [HumanMessage(content=f"Error: {e}", name=name)]}
    
    def _create_supervisor(self, llm, prompt_text, members):
        """Create supervisor"""
        options = ["FINISH"] + members
        function_def = {
            "name": "route",
            "description": "Select next",
            "parameters": {
                "type": "object",
                "properties": {"next": {"anyOf": [{"enum": options}]}},
                "required": ["next"]
            }
        }
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_text),
            MessagesPlaceholder(variable_name="messages"),
            ("system", "Who next? {options}")
        ]).partial(options=str(options))
        
        return prompt | llm.bind_functions(functions=[function_def], function_call="route") | JsonOutputFunctionsParser()
    
    def _setup_agents(self):
        """Setup all agents"""
        self.search_agent = self._create_agent(self.llm, [self.tavily_tool], "Search assistant")
        self.rag_agent = self._create_agent(self.llm, [self.retrieve_info], "RAG assistant")
        self.writer_agent = self._create_agent(self.llm, self.doc_tools, "Writing assistant")
        
        self.research_supervisor = self._create_supervisor(
            self.llm, "Coordinate Search and RAG", ["Search", "RAG"]
        )
        self.doc_supervisor = self._create_supervisor(
            self.llm, "Coordinate writing", ["Writer"]
        )
    
    def _build_graphs(self):
        """Build execution graphs"""
        # Research graph
        class ResState(TypedDict):
            messages: Annotated[List[BaseMessage], operator.add]
            next: str
        
        res_graph = StateGraph(ResState)
        res_graph.add_node("Search", functools.partial(self._agent_node, agent=self.search_agent, name="Search"))
        res_graph.add_node("RAG", functools.partial(self._agent_node, agent=self.rag_agent, name="RAG"))
        res_graph.add_node("supervisor", self.research_supervisor)
        res_graph.add_edge("Search", "supervisor")
        res_graph.add_edge("RAG", "supervisor")
        res_graph.add_conditional_edges("supervisor", lambda x: x["next"], 
                                       {"Search": "Search", "RAG": "RAG", "FINISH": END})
        res_graph.set_entry_point("supervisor")
        
        # Doc graph
        doc_graph = StateGraph(ResState)
        doc_graph.add_node("Writer", functools.partial(self._agent_node, agent=self.writer_agent, name="Writer"))
        doc_graph.add_node("supervisor", self.doc_supervisor)
        doc_graph.add_edge("Writer", "supervisor")
        doc_graph.add_conditional_edges("supervisor", lambda x: x["next"], {"Writer": "Writer", "FINISH": END})
        doc_graph.set_entry_point("supervisor")
        
        self.research_chain = (lambda m: {"messages": [HumanMessage(content=m)]}) | res_graph.compile()
        self.doc_chain = (lambda m: {"messages": [HumanMessage(content=m)]}) | doc_graph.compile()
        
        # Super graph
        super_graph = StateGraph(ResState)
        super_graph.add_node("Research", lambda s: {"messages": [self.research_chain.invoke(s["messages"][-1].content)["messages"][-1]]})
        super_graph.add_node("Docs", lambda s: {"messages": [self.doc_chain.invoke(s["messages"][-1].content)["messages"][-1]]})
        super_graph.add_node("supervisor", self._create_supervisor(self.llm, "Coordinate teams", ["Research", "Docs"]))
        super_graph.add_edge("Research", "supervisor")
        super_graph.add_edge("Docs", "supervisor")
        super_graph.add_conditional_edges("supervisor", lambda x: x["next"], 
                                         {"Research": "Research", "Docs": "Docs", "FINISH": END})
        super_graph.set_entry_point("supervisor")
        
        self.super_graph = super_graph.compile()
    
    def process(self, message: str) -> List[Dict]:
        """Process request"""
        results = []
        try:
            for step in self.super_graph.stream({"messages": [HumanMessage(content=message)]}):
                if "__end__" not in step:
                    results.append(step)
        except Exception as e:
            results.append({"error": str(e)})
        return results
    
    def get_metrics(self) -> Dict:
        """Get comprehensive metrics"""
        return {
            "timestamp": datetime.now().isoformat(),
            "token_metrics": {
                "total": self.token_metrics.total_tokens,
                "prompt": self.token_metrics.prompt_tokens,
                "completion": self.token_metrics.completion_tokens,
                "cached": self.token_metrics.cached_tokens,
                "cost_usd": round(self.token_metrics.estimated_cost, 4),
                "optimization_pct": round(self.token_metrics.get_optimization_percentage(), 1),
                "by_agent": dict(self.token_metrics.agent_breakdown)
            },
            "performance": {
                "requests": self.performance_metrics.total_requests,
                "success_rate": round(self.performance_metrics.get_success_rate(), 1),
                "avg_latency_ms": round(self.performance_metrics.get_avg_latency() * 1000, 0),
                "cache_hit_rate": round(self.performance_metrics.get_cache_hit_rate(), 1),
                "rpm": round(self.performance_metrics.get_requests_per_minute(), 1)
            },
            "system": self.system_monitor.get_current_stats(),
            "gpu": self.gpu_monitor.get_current_stats()
        }


def main():
    """Main entry point"""
    import sys
    
    print("Production Multi-Agent RAG System - Jay Guwalani")
    print("="*60)
    
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("Error: OPENAI_API_KEY required")
        sys.exit(1)
    
    system = ProductionMultiAgentRAG(openai_key, os.getenv("TAVILY_API_KEY"))
    
    if len(sys.argv) > 1:
        msg = " ".join(sys.argv[1:])
        results = system.process(msg)
        print("\nMetrics:")
        print(json.dumps(system.get_metrics(), indent=2))
    else:
        print("\nInteractive mode:")
        while True:
            try:
                inp = input("\n> ")
                if inp.lower() in ['quit', 'exit']:
                    break
                if inp.lower() == 'metrics':
                    print(json.dumps(system.get_metrics(), indent=2))
                elif inp:
                    system.process(inp)
                    print(json.dumps(system.get_metrics(), indent=2))
            except KeyboardInterrupt:
                break


if __name__ == "__main__":
    main()
